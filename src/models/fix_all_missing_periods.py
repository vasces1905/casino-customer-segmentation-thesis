#!/usr/bin/env python3
"""
Fix All Missing Periods - 2022-H1, 2022-H2, 2023-H1
==================================================
Complete missing periods with clean segmentation
"""

import numpy as np
import pandas as pd
import psycopg2
import json
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def convert_numpy_types(obj):
    """Convert numpy types to Python native types for JSON"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(i) for i in obj]
    return obj

def assign_business_label(profile):
    """Assign business labels based on profile"""
    if profile['avg_total_bet'] > 5000 and profile['avg_sessions'] > 10:
        return "High_Value_Player"
    elif profile['avg_loss_chasing'] > 30 or profile['avg_loss_rate'] > 20:
        return "At_Risk_Player"
    elif profile['avg_sessions'] > 3 and profile['avg_total_bet'] > 800:
        return "Regular_Player"
    else:
        return "Casual_Player"

def process_period(period_id, conn):
    """Process a single period"""
    logger.info(f"üîÑ Processing {period_id}...")
    
    # Load clean data
    query = """
    SELECT 
        customer_id,
        total_bet,
        avg_bet,
        loss_rate,
        COALESCE(bet_volatility, 0) as bet_volatility,
        total_sessions,
        COALESCE(loss_chasing_score, 0) as loss_chasing_score
    FROM casino_data.customer_features
    WHERE analysis_period = %s
        AND total_bet > 0
        AND total_bet <= 1500000
        AND avg_bet > 0
    ORDER BY customer_id;
    """
    
    df = pd.read_sql_query(query, conn, params=[period_id])
    
    if len(df) == 0:
        logger.warning(f"‚ö†Ô∏è No data found for {period_id}")
        return False
    
    logger.info(f"‚úÖ Loaded {len(df)} customers for {period_id}")
    
    # Prepare features
    feature_columns = ['total_bet', 'avg_bet', 'loss_rate', 'bet_volatility', 'total_sessions', 'loss_chasing_score']
    X = df[feature_columns].fillna(df[feature_columns].median())
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # KMeans clustering
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    
    # Calculate metrics
    silhouette = silhouette_score(X_scaled, labels)
    davies_bouldin = davies_bouldin_score(X_scaled, labels)
    
    logger.info(f"‚úÖ Clustering completed: Silhouette {silhouette:.4f}")
    
    # Create segment profiles
    df_temp = df.copy()
    df_temp['cluster'] = labels
    
    segment_profiles = {}
    for cluster_id in range(4):
        cluster_data = df_temp[df_temp['cluster'] == cluster_id]
        
        if len(cluster_data) == 0:
            continue
        
        # Safe avg_session_value calculation
        try:
            total_sessions_safe = cluster_data['total_sessions'].replace(0, 1)
            avg_session_value = (cluster_data['total_bet'] / total_sessions_safe).mean()
            if np.isnan(avg_session_value) or np.isinf(avg_session_value):
                avg_session_value = 0.0
        except:
            avg_session_value = 0.0
        
        profile = {
            'cluster_id': cluster_id,
            'size': len(cluster_data),
            'percentage': round(len(cluster_data) / len(df_temp) * 100, 2),
            'avg_total_bet': round(cluster_data['total_bet'].mean(), 2),
            'avg_loss_rate': round(cluster_data['loss_rate'].mean(), 2),
            'avg_session_value': round(avg_session_value, 2),
            'avg_sessions': round(cluster_data['total_sessions'].mean(), 2),
            'avg_loss_chasing': round(cluster_data['loss_chasing_score'].mean(), 2)
        }
        
        profile['business_label'] = assign_business_label(profile)
        segment_profiles[cluster_id] = profile
    
    # Save to database
    cursor = conn.cursor()
    
    # Clear existing data for this period
    cursor.execute("DELETE FROM casino_data.kmeans_segments WHERE period_id = %s AND kmeans_version = 2", (period_id,))
    
    # Insert new data
    for i, customer_id in enumerate(df['customer_id']):
        cluster_id = int(labels[i])
        cluster_profile = segment_profiles.get(cluster_id, {})
        cluster_label = cluster_profile.get('business_label', f'Cluster_{cluster_id}')
        
        # Create metadata
        metadata = convert_numpy_types({
            'algorithm': 'kmeans_clean',
            'silhouette_score': silhouette,
            'davies_bouldin_score': davies_bouldin,
            'data_quality': 'A_GRADE_CLEAN',
            'max_bet_cap': 1500000,
            'academic_approval': 'Bath University A Excellent',
            'timestamp': datetime.now().isoformat()
        })
        
        # Convert segment profile
        segment_data = convert_numpy_types(cluster_profile)
        
        cursor.execute("""
            INSERT INTO casino_data.kmeans_segments (
                customer_id, period_id, cluster_id, cluster_label,
                silhouette_score, distance_to_centroid, model_metadata,
                kmeans_version, segment_data, avg_session_from_metadata
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            int(customer_id),
            period_id,
            cluster_id,
            cluster_label,
            float(silhouette),
            0.0,
            json.dumps(metadata),
            2,  # Clean version
            json.dumps(segment_data),
            float(cluster_profile.get('avg_session_value', 0.0))
        ))
    
    conn.commit()
    logger.info(f"‚úÖ {period_id} segmentation saved!")
    
    # Print segment summary
    print(f"\nüéØ {period_id} SEGMENT SUMMARY:")
    print("="*50)
    for cluster_id, profile in segment_profiles.items():
        print(f"{profile['business_label']}: {profile['size']} customers ({profile['percentage']}%)")
        print(f"  Avg Bet: ‚Ç¨{profile['avg_total_bet']}, Avg Sessions: {profile['avg_sessions']}")
    
    return True

def main():
    """Process all missing periods"""
    
    # Database connection
    conn = psycopg2.connect(
        host="localhost",
        database="casino_research",
        user="researcher",
        password="academic_password_2024"
    )
    
    # Missing periods to process
    missing_periods = ['2022-H1', '2022-H2', '2023-H1']
    
    print("üîß FIXING ALL MISSING PERIODS")
    print("="*50)
    print("Processing: 2022-H1, 2022-H2, 2023-H1")
    print("="*50)
    
    success_count = 0
    for period in missing_periods:
        try:
            if process_period(period, conn):
                success_count += 1
        except Exception as e:
            logger.error(f"‚ùå Failed to process {period}: {e}")
    
    conn.close()
    
    print(f"\nüèÜ PROCESSING COMPLETED!")
    print(f"‚úÖ Successfully processed: {success_count}/{len(missing_periods)} periods")
    
    if success_count == len(missing_periods):
        print("üü¢ ALL PERIODS READY FOR RF TRAINING!")
    else:
        print("üü° SOME PERIODS MISSING - CHECK LOGS")

if __name__ == "__main__":
    main()